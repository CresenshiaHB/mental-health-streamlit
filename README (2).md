# ðŸŽ¬ Netflix Content-Based Recommendation System

This repository contains an end-to-end implementation of a content-based recommendation system for Netflix movies and TV shows. The goal is to recommend the 5 most similar titles based on a user-provided title.

This interactive application is built using **Scikit-learn** for modeling and **Streamlit** for deploying the web interface.

---

## ðŸ“Š Dataset Overview

* **Source:** [Netflix Movies and TV Shows Dataset (Kaggle)](https://www.kaggle.com/datasets/shivamb/netflix-shows)
* **Size:** 8,800+ samples of movies and TV shows.
* **Key Features Used:**

| Feature | Description |
| :--- | :--- |
| `title` | The title of the movie or TV show. |
| `director` | The director of the content. |
| `cast` | A list of the top 3 actors/actresses. |
| `listed_in` (genres) | The category/genre of the content. |
| `description` | A brief synopsis of the content. |
| `type` | The type of content (e.g., Movie or TV Show). |

---

## Methodology & Feature Engineering

This is a pure content-based model. Recommendations are generated by measuring the similarity between titles based on their textual attributes. This is achieved through several preprocessing and feature engineering steps.

### 1. Data Cleaning

The key textual features (director, cast, description) had missing values. The first step was to fill these `NaN` values with an empty string (`''`) to ensure no data was lost during the merging process.

### 2. Feature Extraction

The `cast` and `genres` (from `listed_in`) features are strings containing multiple names/categories. These features were extracted and cleaned to capture the most important components.

* **Cast**: Only the top 3 actors/actresses were taken. Spaces were removed from their names (e.g., "Tom Hanks" becomes "tomhanks") to avoid accidental matches with other names (e.g., "Tom Holland").
* **Genres**: All genres were extracted and formatted in the same way.

```python
df['cast'] = df['cast'].apply(lambda x: [i.strip().lower().replace(' ', '') for i in x.split(',')[:3]])
df['genres'] = df['listed_in'].fillna('').apply(lambda x: [g.strip().lower().replace(' ', '') for g in x.split(',')])
df['director'] = df['director'].apply(lambda x: [x.strip().lower().replace(' ', '')] if x else [])
```

### 3. "Content Soup" Creation

The cleaned features (`genres`, `cast`, `director`, `type`) were combined into a single large string called the "content soup." This string represents all content attributes of a title in a single document.

```python
# 'similarity' is "content soup"
def create_similarity(x):
    return ' '.join(x['genres'] + x['cast'] + x['director']) + ' ' + x['type']

df['similarity'] = df.apply(create_similarity, axis=1)
```

### 4. Vectorization & Similarity Model

The "content soup" for each title was then converted into a numerical vector using `TfidfVectorizer` from Scikit-learn, focusing on the 5,000 most important keywords (features).

Once all titles were represented as vectors, the similarity matrix was calculated using `linear_kernel` (a fast way to compute Cosine Similarity), which compares every title against every other title.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Inisialisasi TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english', max_features=5000)
tfidf_matrix = tfidf.fit_transform(df['similarity'])

# Cosine Similarity Matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
```
---

## Application Architecture (Streamlit)

This web application is built using **Streamlit** (`inferencing.py`) and utilizes a class-based model (`modelling.py`) to keep the code clean and organized.

* `modelling.py`: Contains the NetflixRecommender class, which handles all data loading, preprocessing, and modeling logic.
* `inferencing.py`: Contains the Streamlit UI logic.
* `netflix_df.pkl`: A pickle file of the preprocessed DataFrame (after steps 1 & 2) to speed up the application's startup time.

On startup, the app loads `netflix_df.pkl`, and then the `NetflixRecommender` class builds the TF-IDF model and similarity matrix in memory.

```python
@st.cache_resource
def load_model():
    # Load DataFrame yang sudah diproses
    with open("netflix_df.pkl", "rb") as f:
        df = pickle.load(f)
    
    # Inisialisasi model dan inject df
    model = NetflixRecommender("dummy.csv")
    model.df = df
    model.build_model() # Membangun TF-IDF & Cosine Sim
    return model

recommender = load_model()

if st.button("Show recommendation") and selected_title:
    # Panggil model untuk mendapatkan rekomendasi
    result = recommender.get_recommendations(selected_title, topn=5)
```
---

## Tech Stack
* **Python**
* **Pandas**: For data manipulation and preprocessing.
* **Scikit-learn**: For TfidfVectorizer and linear_kernel (Cosine Similarity).
* **Streamlit**: For building and serving the interactive web interface.
* **Pickle**: For serializing and deserializing the processed DataFrame.

---

## How to Run Locally
1. Clone the repository:
   ```python
   git clone [https://github.com/CresenshiaHB/model-deployment-project.git](https://github.com/CresenshiaHB/model-deployment-project.git)
   cd model-deployment-project
   ```
2. Create a virtual environment (recommended):
   ```python
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install the dependencies:
   ```python
   pip install -r requirements.txt
   ```
4. Run the Streamlit app:
   ```python
   streamlit run inferencing.py
   ```
5. Pada browser buka link berikut http://localhost:8501.
